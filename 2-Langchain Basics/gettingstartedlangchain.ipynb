{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os # for os.path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langsmith Tracking and Tracing\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10d51aa80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10d5c3350> root_client=<openai.OpenAI object at 0x10c8d67e0> root_async_client=<openai.AsyncOpenAI object at 0x10d36e690> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is the capital of France?\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user is asking, \"What is the capital of France?\" Let me think about how to approach this.\\n\\nFirst, I need to confirm the answer. The capital of France is Paris. But wait, maybe I should double-check to be sure. Sometimes there can be confusion, especially with countries that have historical changes. For example, some countries have had their capitals moved due to political reasons. But France\\'s capital has been Paris for a long time. \\n\\nI should also consider if there\\'s any recent change that I\\'m not aware of. A quick mental recap: France is a well-known country, and Paris is its most famous city. It\\'s the political, cultural, and economic center. The government is there, including the presidency and parliament. \\n\\nHmm, could there be any trick in the question? Maybe they\\'re looking for something else, but I don\\'t think so. The question is straightforward. The user might be a student needing the answer for homework, or someone learning basic geography. \\n\\nAlternatively, maybe they want more details, like when Paris became the capital. But the question specifically asks for the capital, so the answer is Paris. I should keep it concise but accurate. \\n\\nWait, sometimes people confuse Paris with other French cities, like Lyon or Marseille, but no, those are different. Paris is definitely the capital. \\n\\nAnother angle: some countries have multiple capitals for different branches of government, but France isn\\'t one of them. The capital is solely Paris. \\n\\nSo the answer is straightforward. I should present it clearly, maybe with a brief explanation if needed, but the user just asked for the capital. Keep it simple.\\n</think>\\n\\nThe capital of France is **Paris**. It has been the political, cultural, and historical heart of the country for centuries, serving as the seat of the French government and home to iconic landmarks like the Eiffel Tower and the Louvre Museum.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 389, 'prompt_tokens': 17, 'total_tokens': 406, 'completion_time': 0.959529453, 'prompt_time': 0.003106635, 'queue_time': 0.016591835, 'total_time': 0.962636088}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_1e88ca32eb', 'finish_reason': 'stop', 'logprobs': None}, id='run--954281e6-f640-4614-aa7a-134505ac03bc-0', usage_metadata={'input_tokens': 17, 'output_tokens': 389, 'total_tokens': 406})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "\n",
    "model.invoke(\"What is the capital of France?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10dda2ae0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1189c6a20>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model1=ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x10dda2ae0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1189c6a20>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chaining \n",
    "\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're asking about **Langsmith**, which is a fantastic project! \n",
      "\n",
      "Langsmith is an open-source platform designed to make it easier to build and deploy large language models (LLMs). Think of it as a toolbox packed with helpful utilities specifically for working with LLMs. \n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "* **Democratizing LLM Development:** Langsmith aims to lower the barrier to entry for building and experimenting with LLMs. It provides a streamlined environment that doesn't require extensive technical expertise.\n",
      "* **Python-First Approach:**  It's built with Python in mind, making it accessible to a wide range of developers.\n",
      "\n",
      "* **Modular Design:** Langsmith is modular, meaning you can easily integrate different components and customize your workflow.\n",
      "\n",
      "* **Streamlined Experimentation:** It simplifies the process of training, evaluating, and fine-tuning LLMs. You can quickly iterate on your models and see the results.\n",
      "\n",
      "* **Community-Driven:** Langsmith is an open-source project, which means it benefits from the contributions and expertise of a vibrant community of developers.\n",
      "\n",
      "**Here's why Langsmith is gaining attention:**\n",
      "\n",
      "* **Ease of Use:**  It makes working with LLMs more approachable for beginners and experts alike.\n",
      "* **Flexibility:** You can adapt Langsmith to your specific needs and use cases.\n",
      "* **Collaboration:** The open-source nature fosters collaboration and knowledge sharing.\n",
      "\n",
      "**If you're interested in exploring Langsmith further, I recommend checking out their:**\n",
      "\n",
      "* **Website:** [https://github.com/langsmithai/langsmith](https://github.com/langsmithai/langsmith)\n",
      "* **Documentation:** [https://docs.langsmith.ai/](https://docs.langsmith.ai/)\n",
      "* **Community Forum:** [https://discord.gg/P8mK6zK3q4](https://discord.gg/P8mK6zK3q4)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or LLMs in general!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response= chain.invoke({\"input\": \"Can you tell me something about the Langsmith?\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, you're asking about Langsmith! It's a fascinating project. Let me tell you what I know:\n",
      "\n",
      "**Langsmith** is an open-source platform developed by the Gemma team at Google DeepMind. It's designed to streamline the process of building and deploying large language models (LLMs). Think of it as a toolbox specifically for working with LLMs, offering a range of features and tools to make the process more efficient and accessible.\n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "* **Focus on Efficiency:** Langsmith emphasizes building and using LLMs efficiently. It offers tools for fine-tuning existing models, allowing you to customize them for specific tasks without needing to train them from scratch, which can be incredibly resource-intensive.\n",
      "* **Open-Source and Collaborative:** As an open-source project, Langsmith thrives on community contributions. This means you can benefit from the work of others, share your own tools and improvements, and collaborate with a global network of researchers and developers.\n",
      "* **User-Friendly Interface:**  Langsmith aims to make working with LLMs more approachable for everyone. It provides a web-based interface that's relatively easy to use, even if you're not a seasoned AI engineer.\n",
      "\n",
      "**How does it work?**\n",
      "\n",
      "Essentially, Langsmith provides a framework for:\n",
      "\n",
      "* **Model Management:**  Easily load, manage, and version different LLMs.\n",
      "* **Fine-Tuning:**  Adapt existing models to your specific needs through fine-tuning techniques.\n",
      "* **Experimentation:**  Run experiments and compare different model architectures, training parameters, and fine-tuning strategies.\n",
      "* **Deployment:**  Deploy your fine-tuned models for real-world applications.\n",
      "\n",
      "**Why is it important?**\n",
      "\n",
      "Langsmith democratizes access to LLM technology. By simplifying the development and deployment process, it empowers more people and organizations to leverage the power of LLMs for various applications, from research and education to creative writing and business automation.\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about Langsmith or anything else related to AI!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Output Parsers\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain= prompt|model|output_parser\n",
    "\n",
    "response= chain.invoke({\"input\": \"can you tell me something about the Langsmith?\"})\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Output Parser using JSON\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"answer the query \\n {format_instruction} \\n {query}\\n\",\n",
    "    partial_variables={\"format_instruction\": output_parser.get_format_instructions()},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='answer the query \\n {format_instruction} \\n {query}\\n')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for developing and deploying AI assistants. ', 'features': ['Allows users to build, train, and fine-tune their own language models.', 'Provides a library of pre-trained models for various tasks.', 'Offers tools for evaluating and monitoring model performance.', 'Enables seamless integration with other AI tools and services.'], 'creator': 'Hugging Face', 'website': 'https://www.huggingface.co/blog/introducing-langsmith'}\n"
     ]
    }
   ],
   "source": [
    "chain= prompt|model|output_parser\n",
    "response= chain.invoke({\"query\": \"can you tell me something about the Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Assignment  -- chatPromptTemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"Langsmith is an open-source platform developed by the Hugging Face community for building and deploying large language models (LLMs). It simplifies the process of fine-tuning and using pre-trained LLMs for various downstream tasks, such as text generation, summarization, and question answering.\\n\\n  Here are some key features of Langsmith:\\n\\n  * **User-friendly Interface:** Langsmith provides a web-based interface that makes it easy to interact with LLMs, even for users without extensive coding experience.\\n\\n  * **Fine-tuning Capabilities:** It allows users to fine-tune pre-trained LLMs on their own datasets, enabling them to customize the model's performance for specific applications.\\n\\n  * **Model Management:** Langsmith offers tools for managing and versioning different LLM models, facilitating collaboration and reproducibility.\\n\\n  * **Pipeline Creation:** Users can build and deploy pipelines that chain together multiple LLMs and other tools to create complex workflows.\\n\\n  * **Community Collaboration:** As an open-source project, Langsmith benefits from the contributions and support of a large community of developers and researchers.\\n\\n  Langsmith aims to democratize access to LLMs and empower individuals and organizations to leverage their potential for various applications.\"}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in XML markup format with <output>  tags.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser using XML\n",
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in XML markup format with <output>  tags.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{query}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': \"\\n\\nLangsmith is an open-source platform developed by Replicate that simplifies the process of training and deploying large language models (LLMs). \\n\\nHere are some key features of Langsmith:\\n\\n* **User-Friendly Interface:** Langsmith provides a web-based interface that makes it accessible to both developers and non-developers.\\n* **Streamlined Training Process:** It offers tools and workflows to streamline the training of LLMs, including data preparation, model selection, and hyperparameter tuning.\\n* **Model Deployment:** Langsmith enables easy deployment of trained LLMs as APIs, allowing developers to integrate them into their applications.\\n* **Community-Driven:** As an open-source platform, Langsmith benefits from a vibrant community of contributors who contribute to its development and support.\\n* **Integration with Replicate:** Langsmith is tightly integrated with Replicate, a platform for hosting and running machine learning models.\\n\\n**Benefits of using Langsmith:**\\n\\n* **Reduced Development Time:** Langsmith's tools and automation features accelerate the development process for LLMs.\\n* **Improved Accessibility:** The user-friendly interface and open-source nature make LLMs more accessible to a wider range of users.\\n* **Cost-Effectiveness:** By simplifying the training and deployment process, Langsmith can help reduce development costs.\\n* **Flexibility:** Langsmith supports a variety of LLM architectures and frameworks, providing flexibility for different use cases.\\n\\nLangsmith is a valuable resource for anyone interested in exploring and leveraging the power of large language models.\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides a set of tools and building blocks to help developers create, manage, and interact with LLMs in a more efficient and flexible way. \\n\\nHere are some key features of LangChain:\\n\\n- **Chain creation:** Allows you to build chains of LLMs and other components (like search engines or databases) to perform complex tasks.\\n- **Memory management:** Provides tools for managing and persisting the context of conversations with LLMs.\\n- **Prompt templates:** Offers a way to define and reuse prompts for different use cases.\\n- **Agents:** Enables the creation of autonomous agents that can interact with the world through APIs and other tools.\\n\\nOverall, LangChain aims to make it easier for developers to leverage the power of LLMs in a wide range of applications.</answer></response>\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 39, 'total_tokens': 233, 'completion_time': 0.352727273, 'prompt_time': 0.002371005, 'queue_time': 0.10807230200000001, 'total_time': 0.355098278}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--f0602fa7-0378-4352-8e60-4c304f9ac9e1-0' usage_metadata={'input_tokens': 39, 'output_tokens': 194, 'total_tokens': 233}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why couldn't the bicycle stand up by itself? It was two tired!\"}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>The Green Mile</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>Philadelphia</movie>\n",
      "<movie>Sully</movie>\n",
      "<movie>Toy Story</movie>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product_name': 'Audi A6', 'price': 'Starting at $55,900', 'details': {'engine': '2.0L TFSI I4 engine', 'horsepower': '261 hp', 'transmission': '7-speed S tronic automatic', 'drivetrain': 'quattro all-wheel drive', 'fuel_economy': {'city': '24 mpg', 'highway': '31 mpg'}, 'interior_features': ['Leather seating surfaces', '3-zone automatic climate control', 'Audi virtual cockpit'], 'technology': ['MMI touch response with dual center displays', 'Audi smartphone interface', 'Bang & Olufsen 3D sound system'], 'safety_features': ['Audi pre sense basic and city', 'Lane departure warning', 'Rear view camera'], 'dimensions': {'length': '194.2 inches', 'wheelbase': '115.1 inches', 'cargo_capacity': '13.7 cubic feet'}, 'warranty': '4-year/50,000-mile limited warranty'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pydantic class for the output\n",
    "class ProductDetails(BaseModel):\n",
    "    product_name: str = Field(description=\"Name of the product\")\n",
    "    price: int = Field(description=\"Price in USD\")\n",
    "    details: str = Field(description=\"Details of the product\")\n",
    "\n",
    "\n",
    "# Output Parser\n",
    "\n",
    "parser= JsonOutputParser(pydantic_object=ProductDetails)\n",
    "\n",
    "# Chat Prompt Template\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an Product Expert. Provide the product details in JSON format, including product name, price, and details.\"),\n",
    "        (\"user\", \"{input}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model\n",
    "model= ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Chaining\n",
    "chain= prompt|model|parser\n",
    "\n",
    "# Invoke the chain\n",
    "response= chain.invoke({\"input\": \"provide the details about the audi a6\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product_name': 'iPhone 16 Pro Max', 'price': '1399 USD', 'details': {'display': '6.9-inch Super Retina XDR', 'processor': 'A19 Bionic chip', 'storage_options': ['256GB', '512GB', '1TB', '2TB'], 'camera_system': 'Triple 48MP Ultra-Wide, Wide, and Telephoto cameras', 'battery_life': 'Up to 28 hours talk time', 'design': 'Ceramic Shield front, Textured matte glass back, and Surgical-grade stainless steel frame', 'operating_system': 'iOS 18', 'features': ['5G capable', 'ProMotion technology with adaptive refresh rates up to 120Hz', 'Face ID', 'MagSafe technology', 'Improved Night mode and Smart HDR'], 'color_options': ['Graphite', 'Gold', 'Silver', 'Sierra Blue']}}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain\n",
    "response= chain.invoke({\"input\": \"provide the details about iphone 16 pro max\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
